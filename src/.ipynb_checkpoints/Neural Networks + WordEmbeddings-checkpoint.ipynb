{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4a6d617e",
      "metadata": {
        "id": "4a6d617e"
      },
      "source": [
        "#### Load required libs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "af64d6e1",
      "metadata": {
        "id": "af64d6e1"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "import nltk\n",
        "\n",
        "from scipy.sparse import coo_matrix\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.neural_network import MLPClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3e8f013",
      "metadata": {
        "id": "a3e8f013"
      },
      "source": [
        "### Load data\n",
        "\n",
        "Dowload data from https://github.com/clinc/oos-eval/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "840d2d69",
      "metadata": {
        "id": "840d2d69",
        "outputId": "53b067c4-c8d6-4cc0-fdb0-0f05df5b8abc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-2ee1a6a745db>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/data_small.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/data_small.json'"
          ]
        }
      ],
      "source": [
        "f = open('../data/data_small.json')\n",
        "\n",
        "data = json.load(f)\n",
        "\n",
        "print(data.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a405dff1",
      "metadata": {
        "id": "a405dff1"
      },
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f62f4b57",
      "metadata": {
        "id": "f62f4b57"
      },
      "outputs": [],
      "source": [
        "len(data['train'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "506ae835",
      "metadata": {
        "id": "506ae835"
      },
      "outputs": [],
      "source": [
        "def get_dataset(data, dataset='train'):\n",
        "    X = []\n",
        "    Y = []\n",
        "    for sample in data[dataset]:\n",
        "        X.append(sample[0])\n",
        "        Y.append(sample[1])\n",
        "        \n",
        "    return X, Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdbafb8e",
      "metadata": {
        "id": "cdbafb8e"
      },
      "outputs": [],
      "source": [
        "X_raw = {}\n",
        "Y = {}\n",
        "\n",
        "datasets = ['train', 'val', 'test']\n",
        "\n",
        "for dataset in datasets:\n",
        "    X_raw[dataset], Y[dataset] = get_dataset(data, dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab2ee18d",
      "metadata": {
        "id": "ab2ee18d"
      },
      "source": [
        "### Download Word Embeddings model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03930d19",
      "metadata": {
        "id": "03930d19"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader\n",
        "\n",
        "print(list(gensim.downloader.info()['models'].keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c95c1394",
      "metadata": {
        "id": "c95c1394"
      },
      "outputs": [],
      "source": [
        "word_embeddings = gensim.downloader.load('word2vec-google-news-300')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0249009f",
      "metadata": {
        "id": "0249009f"
      },
      "outputs": [],
      "source": [
        "word_embeddings['banana']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2e75b1c",
      "metadata": {
        "id": "a2e75b1c"
      },
      "outputs": [],
      "source": [
        "word_embeddings['banana'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e85c15e",
      "metadata": {
        "id": "1e85c15e"
      },
      "outputs": [],
      "source": [
        "word_embeddings.most_similar('banana')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0a9063a",
      "metadata": {
        "id": "b0a9063a"
      },
      "source": [
        "### Convert raw texts to Averaged Word Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33242b96",
      "metadata": {
        "id": "33242b96"
      },
      "outputs": [],
      "source": [
        "def raw2embeddings(texts, word_embeddings):\n",
        "    \n",
        "    dim, = word_embeddings['the'].shape\n",
        "    \n",
        "    X = []\n",
        "    for text in texts:\n",
        "        tokens = nltk.word_tokenize(text)\n",
        "        \n",
        "        embeddings = [ word_embeddings[token] for token in tokens if token in word_embeddings ]\n",
        "        \n",
        "        if len(embeddings) > 0:\n",
        "            X.append( np.mean(embeddings, axis=0) ) # ignoring Out-of-vocabulary (OOV) words\n",
        "        else:\n",
        "            X.append( np.zeros((dim,)))\n",
        "        \n",
        "        \n",
        "    return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c1cd59e",
      "metadata": {
        "id": "0c1cd59e"
      },
      "outputs": [],
      "source": [
        "X = {}\n",
        "\n",
        "for dataset in datasets:\n",
        "    print(dataset)\n",
        "    X[dataset] = raw2embeddings(X_raw[dataset], word_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1722c555",
      "metadata": {
        "id": "1722c555"
      },
      "outputs": [],
      "source": [
        "X_raw['train'][:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b244845",
      "metadata": {
        "id": "4b244845"
      },
      "outputs": [],
      "source": [
        "X['train'][:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7008906b",
      "metadata": {
        "id": "7008906b"
      },
      "source": [
        "## Train and evaluate classifiers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df0c0f09",
      "metadata": {
        "id": "df0c0f09"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate_classifier(clf, X_train, Y_train, X_test, Y_test):\n",
        "    clf.fit(X_train, Y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    \n",
        "    print(f'Accuracy: {accuracy_score(Y_test, y_pred)}')\n",
        "    \n",
        "    y_probs = clf.predict_proba(X_test)[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bedc4ad4",
      "metadata": {
        "id": "bedc4ad4"
      },
      "outputs": [],
      "source": [
        "train_and_evaluate_classifier( LogisticRegression(random_state=0), X['train'], Y['train'], X['test'], Y['test'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5efb772e",
      "metadata": {
        "id": "5efb772e"
      },
      "outputs": [],
      "source": [
        "train_and_evaluate_classifier( MLPClassifier(random_state=1, max_iter=300, early_stopping=True, verbose=True), X['train'], Y['train'], X['test'], Y['test'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6a283c3",
      "metadata": {
        "id": "e6a283c3"
      },
      "source": [
        "### Train and classify new samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db8e50cb",
      "metadata": {
        "id": "db8e50cb"
      },
      "outputs": [],
      "source": [
        "clf = MLPClassifier(random_state=1, max_iter=300, early_stopping=True, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c541b9f",
      "metadata": {
        "scrolled": true,
        "id": "3c541b9f"
      },
      "outputs": [],
      "source": [
        "clf.fit( X['train'], Y['train'] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5f1eb21",
      "metadata": {
        "id": "b5f1eb21"
      },
      "outputs": [],
      "source": [
        "data['test'][:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9179a851",
      "metadata": {
        "id": "9179a851"
      },
      "outputs": [],
      "source": [
        "#text = 'Could you bring me more info about direct deposits?'\n",
        "#text = 'how would you say fly in italian'\n",
        "text = 'tell me about chatgpt'\n",
        "\n",
        "print(clf.predict( raw2embeddings([text], word_embeddings) ))\n",
        "sorted( zip( clf.classes_, clf.predict_proba( raw2embeddings([text], word_embeddings) )[0] ), key=lambda x:x[1], reverse=True )[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25ad6e15",
      "metadata": {
        "id": "25ad6e15"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}